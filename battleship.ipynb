{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code is written by EFavDB\n",
    "I only walked through it, played with parameters, and added comments.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BOARD_SIZE = 20\n",
    "SHIP_SIZE = 3\n",
    "\n",
    "hidden_units = BOARD_SIZE\n",
    "output_units = BOARD_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy network to inference action given a current state\n",
    "\n",
    "input_positions = tf.placeholder(tf.float32, (1,BOARD_SIZE))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# The input neuron number is equal to the game pixel\n",
    "w1 = tf.Variable(tf.truncated_normal([BOARD_SIZE, hidden_units]))\n",
    "b1 = tf.Variable(tf.zeros([hidden_units]))\n",
    "h1 = tf.tanh(tf.matmul(input_positions, w1) + b1)\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([hidden_units, output_units]))\n",
    "b2 = tf.Variable(tf.zeros([output_units]))\n",
    "logits = tf.matmul(h1, w2) + b2\n",
    "\n",
    "# The output is the probability of each action\n",
    "probabilities = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training of the policy network is faciliated by learning_rate induced by reward function defined below\n",
    "\n",
    "# here labels = x in range(10). Not one-hot. So we use sparse xentropy below\n",
    "labels = tf.placeholder(tf.int64)\n",
    "\n",
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='xentropy')\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def play_game(sess, training=True):\n",
    "    \"\"\" Play a single game of battleship using a fixed policy network.\n",
    "    \n",
    "    Args:\n",
    "        session: tensorflow session\n",
    "        training: boolean to indicate training\n",
    "    \n",
    "    Returns:\n",
    "        board_position_log [[int]]: history of gameplay e.g. [[-1,-1, ... -1],[-1,1, ... -1],[0,1, ... -1], ...]\n",
    "        action_log [int]: history of bombing position 0 to 9 e.g. [1,0,6, ...]\n",
    "        hit_log [int]: history of hit 1 or not hit 0 in the game e.g. [1,0,0, ...]\n",
    "    \n",
    "    \"\"\"\n",
    "    # Select random location for ship\n",
    "    ship_left = np.random.randint(BOARD_SIZE - SHIP_SIZE + 1)\n",
    "    ship_positions = set(range(ship_left, ship_left + SHIP_SIZE))\n",
    "    # Initialize logs for game\n",
    "    board_position_log = []\n",
    "    action_log = []\n",
    "    hit_log = []\n",
    "    # Play through game\n",
    "    current_board = [[-1 for i in range(BOARD_SIZE)]]\n",
    "    board_position_log.append([[i for i in current_board[0]]])\n",
    "    \n",
    "    while (sum(hit_log) < SHIP_SIZE) and (len(action_log) < BOARD_SIZE):\n",
    "        feed_dict = {\n",
    "            input_positions : current_board\n",
    "        }\n",
    "        # here, we use the current policy network for inference\n",
    "        # the action to be taken is drawn by probs\n",
    "        probs = sess.run([probabilities], feed_dict=feed_dict)[0][0]\n",
    "        probs = [p * (index not in action_log) for index, p in enumerate(probs)]\n",
    "        probs = [p / sum(probs) for p in probs]\n",
    "        if training == True:\n",
    "            bomb_index = np.random.choice(BOARD_SIZE, p=probs)            \n",
    "        else:\n",
    "            bomb_index = np.argmax(probs)\n",
    "        # update board, logs\n",
    "        hit_log.append(1 * (bomb_index in ship_positions))\n",
    "        current_board[0][bomb_index] = 1 * (bomb_index in ship_positions)\n",
    "        board_position_log.append([[i for i in current_board[0]]])\n",
    "        action_log.append(bomb_index)\n",
    "    return board_position_log, action_log, hit_log\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward function\n",
    "\n",
    "$r(a;t_0) = \\sum_{t>t_0} (h(t) - \\bar h(t)) \\gamma^{(t-t_0)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_rewards(hit_log, gamma=0.5):\n",
    "    \"\"\" Reward function of one completed game\n",
    "    \n",
    "    Args:\n",
    "        hit_log [int]: history of hit 1 or not hit 0 in the game e.g. [1,0,0, ...]\n",
    "        gamma (float<1): diminishing return parameter to suppress postponed hits\n",
    "    \n",
    "    Return:\n",
    "    \"\"\"\n",
    "    # for one completed game, we re-weight the hit score at each time step (originaly 1 for hit or 0 for not hit) by\n",
    "    #    subtracting the average score beyond that time step; and \n",
    "    #    multiply a diminishing reward factor for hits beyond that time step\n",
    "    hit_log_weighted = [\n",
    "            (\n",
    "                item\n",
    "                - float(SHIP_SIZE - sum(hit_log[:index])) / float(BOARD_SIZE - index)\n",
    "            )\n",
    "            * (gamma ** index)\n",
    "        for index, item in enumerate(hit_log)\n",
    "    ]\n",
    "    # for one completed game, the reward for each time step is the sum over the future weighted hit\n",
    "    rewards = [\n",
    "            sum(hit_log_weighted[index:])\n",
    "            * (gamma ** (- index))\n",
    "        for index in range(len(hit_log))\n",
    "    ]\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: [ 3.54401755]; game length: 14; action history: [10, 18, 0, 9, 2, 16, 12, 6, 5, 3, 13, 17, 1, 11]\n",
      "cost: [ 0.10565664]; game length: 14; action history: [5, 3, 12, 11, 2, 14, 6, 16, 7, 17, 9, 10, 15, 13]\n",
      "cost: [ 0.08510733]; game length: 16; action history: [16, 5, 3, 9, 15, 8, 6, 14, 2, 17, 10, 12, 11, 13, 1, 0]\n",
      "cost: [ 0.06959347]; game length: 8; action history: [12, 8, 5, 2, 3, 1, 6, 4]\n",
      "cost: [ 0.02949222]; game length: 6; action history: [12, 16, 11, 9, 5, 13]\n",
      "cost: [ 0.00756459]; game length: 7; action history: [12, 8, 5, 17, 2, 18, 16]\n",
      "cost: [ 0.18464011]; game length: 10; action history: [8, 9, 12, 16, 14, 2, 17, 5, 3, 4]\n",
      "cost: [ 0.02112601]; game length: 4; action history: [8, 7, 10, 9]\n",
      "cost: [ 0.00102634]; game length: 7; action history: [3, 16, 8, 9, 7, 5, 6]\n",
      "cost: [ 0.00338767]; game length: 6; action history: [2, 10, 12, 11, 16, 9]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    game_lengths = []\n",
    "    ALPHA = 0.06  # global training rate. This is to be multipled by reward function of each step\n",
    "    \n",
    "    # play 10000 games\n",
    "    for i in range(10000):\n",
    "        board_position_log, action_log, hit_log = play_game(sess)\n",
    "        rewards_log = calculate_rewards(hit_log)\n",
    "        game_lengths.append(len(action_log))\n",
    "        \n",
    "        # for each action in each completed game, train the policy network by feedbacking reward values\n",
    "        for r,b,a in zip(rewards_log, board_position_log[:-1], action_log):\n",
    "            feed_dict = {\n",
    "                input_positions: b,\n",
    "                labels: [a],\n",
    "                learning_rate: ALPHA * r  # negative learn rate for negative reward\n",
    "            }\n",
    "            sess.run(train_step, feed_dict=feed_dict)\n",
    "            \n",
    "        if i%1000 == 0:\n",
    "            print(\"cost: {}; game length: {}; action history: {}\".format(\n",
    "                    sess.run(cost, feed_dict=feed_dict), \n",
    "                    len(action_log),\n",
    "                    action_log\n",
    "                ))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comments\n",
    "\n",
    "<p>Possible issue:</p>\n",
    "<ol>\n",
    "  <li>diverge for large system.</li>\n",
    "  <li>overfit in high dimension.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
